{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook exands on the first notebook\n",
    " - Comments are pruned\n",
    " - More advanced NLP techniques are used during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Tensorflow version information\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check GPU presence\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import raw reviews and labels\n",
    "with open('./input_data/movie_reviews/reviews.txt', 'r') as f:\n",
    "    reviews_raw = f.read()\n",
    "with open('./input_data/movie_reviews/labels.txt', 'r') as f:\n",
    "    labels_raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review preprocessing\n",
    " - tokenize\n",
    " - remove punctuation\n",
    " - remove stopwords\n",
    " - perform stemming\n",
    " - split into individual reviews\n",
    " - convert to lowercase\n",
    " - create list of words for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizeContent(contentsRaw):\n",
    "    tokenized = nltk.tokenize.word_tokenize(contentsRaw)\n",
    "    return tokenized\n",
    "\n",
    "def removeStopWordsFromTokenized(contentsTokenized, stop_word_set):\n",
    "    filteredContents = [word for word in contentsTokenized if word not in stop_word_set]\n",
    "    return filteredContents\n",
    "\n",
    "def performPorterStemmingOnContents(contentsTokenized, porterStemmer):\n",
    "    filteredContents = [porterStemmer.stem(word) for word in contentsTokenized]\n",
    "    return filteredContents\n",
    "\n",
    "def removePunctuationFromTokenized(contentsTokenized, excludePuncuation):\n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents\n",
    "\n",
    "def convertItemsToLower(contentsRaw):\n",
    "    filteredContents = [term.lower() for term in contentsRaw]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap preprocessing functions into a convenience function\n",
    "# this done becuase order *is* important\n",
    "def processData(rawContents, stop_word_set, porterStemmer, excludePuncuation):\n",
    "    cleaned = tokenizeContent(rawContents)\n",
    "    cleaned = removeStopWordsFromTokenized(cleaned, stop_word_set)\n",
    "    cleaned = removePunctuationFromTokenized(cleaned, excludePuncuation)\n",
    "    cleaned = performPorterStemmingOnContents(cleaned, porterStemmer)    \n",
    "    cleaned = convertItemsToLower(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into individual reviews (they are delimited by a '\\n')\n",
    "reviews_list = reviews_raw.split('\\n')\n",
    "# this leaves a \"\" empty value in the last index\n",
    "# sample: print(len(reviews)) > 25001 == \"\"\n",
    "# delete this last empty value\n",
    "del reviews_list[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_reviews_list = []\n",
    "stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "porterStemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "excludePuncuation = set(string.punctuation)\n",
    "# manually add additional punctuation to remove\n",
    "excludePuncuation.add('\\'\\'')\n",
    "excludePuncuation.add('--')\n",
    "excludePuncuation.add('``')\n",
    "for rev in reviews_list:\n",
    "    processed_reviews_list.append(processData(rev, stop_word_set, porterStemmer, excludePuncuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for r in processed_reviews_list:\n",
    "    for w in r:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "# output sample| print(word_counts) > 'Counter({'the': 336713, 'and': 164107, 'a': 163009, ....'\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# output sample| print(vocab) > '['the', 'and', 'a', 'of', 'to','\n",
    "\n",
    "# convert vocab to int\n",
    "# NOTE: start at 1, not 0\n",
    "vocab_to_int = {word: maping_int for maping_int, word in enumerate(vocab, 1) }\n",
    "# output sample| print(vocab_to_int) > ''together': 291, 'ewing': 26224,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "reviews_as_ints = []\n",
    "for r in processed_reviews_list:\n",
    "    reviews_as_ints.append([vocab_to_int[term] for term in r])\n",
    "print(len(reviews_as_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare reviews (raw to converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review:\n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "------------------\n",
      "Processed Review:\n",
      "['stori', 'man', 'unnatur', 'feel', 'pig', 'start', 'open', 'scene', 'terrif', 'exampl', 'absurd', 'comedi', 'formal', 'orchestra', 'audienc', 'turn', 'insan', 'violent', 'mob', 'crazi', 'chant', 'singer', 'unfortun', 'stay', 'absurd', 'whole', 'time', 'gener', 'narr', 'eventu', 'make', 'put', 'even', 'era', 'turn', 'cryptic', 'dialogu', 'would', 'make', 'shakespear', 'seem', 'easi', 'third', 'grader', 'technic', 'level', 'better', 'might', 'think', 'good', 'cinematographi', 'futur', 'great', 'vilmo', 'zsigmond', 'futur', 'star', 'salli', 'kirkland', 'freder', 'forrest', 'seen', 'briefli']\n",
      "------------------\n",
      "Review with terms mapped to ints:\n",
      "[13, 55, 5318, 62, 2750, 86, 246, 18, 1140, 357, 1265, 105, 6843, 5751, 177, 94, 1459, 957, 2508, 794, 6269, 1413, 352, 434, 1265, 143, 6, 256, 1153, 703, 8, 139, 14, 858, 94, 9710, 334, 15, 8, 1605, 39, 686, 710, 6390, 1024, 448, 58, 155, 30, 7, 563, 613, 26, 20839, 20523, 613, 76, 2868, 8670, 12241, 5393, 47, 2872]\n"
     ]
    }
   ],
   "source": [
    "# compare\n",
    "print(\"Raw review:\")\n",
    "print(reviews_list[1])\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Processed Review:\")\n",
    "print(processed_reviews_list[1])\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Review with terms mapped to ints:\")\n",
    "print(reviews_as_ints[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the current state/information about our data\n",
    "\n",
    "# TODO: this visualization logic may be flawed - review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number reviews: 25000\n",
      "Zero-length reviews: 0\n",
      "Avg review length: 123.55172\n",
      "Maximum review length: 1442\n"
     ]
    }
   ],
   "source": [
    "# review_lengths = Counter([len(each_review) for each_review in reviews_as_ints])\n",
    "\n",
    "# review_len_list = list(reviews_as_ints)\n",
    "# rl_sorted = sorted(review_len_list)\n",
    "# num_reviews = len(rl_sorted)\n",
    "# avg_len = sum(val * review_lengths[val] for val in review_lengths) / num_reviews\n",
    "\n",
    "# print(\"Number reviews: {}\".format(num_reviews))\n",
    "# print(\"Zero-length reviews: {}\".format(review_lengths[0]))\n",
    "# print(\"Avg review length: {}\".format(avg_len))\n",
    "# print(\"Maximum review length: {}\".format(max(review_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b5613ca0b908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_sorted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JackB\\AppData\\Local\\conda\\conda\\envs\\tf_v1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2888\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2889\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\JackB\\AppData\\Local\\conda\\conda\\envs\\tf_v1\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# ### Getting a closer look at the data\n",
    "# import numpy as np\n",
    "# import scipy.stats as stats\n",
    "# import pylab as pl\n",
    "\n",
    "# fit = stats.norm.pdf(rl_sorted, np.mean(rl_sorted), np.std(rl_sorted))\n",
    "\n",
    "# pl.plot(rl_sorted,fit,'-o')\n",
    "# pl.hist(rl_sorted, normed=True)\n",
    "# pl.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential issues:\n",
    "1. The max movie length is long (1442)\n",
    "\n",
    "### Both of these problems could be addressed by trimming the reviews\n",
    " - Will trim to `seq_len`\n",
    "\n",
    "NOTE: There is a cost to trimming our data -- we're losing some of the information of each review over n length.  This isn't ideal.  Another solution could involve removing the excessively large reviews from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0 15691   235   732   105  1920     6  1329   314    50  1294    43\n",
      "   1533  3767   175    93 15691   235  1298    21  2123   518  1294  9099\n",
      "    866  2803  1509   733    11   112  1067  1294 14695  3987   143   456\n",
      "    611   314   624   733   134   180   733  3141    54   946   314   983\n",
      "   1666   235   237   116  2493  5118     4  1294   733  1745 15691   235\n",
      "    146    46   630   318    30 15691   235   150  2856  1490]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0    13    55  5318    62  2750\n",
      "     86   246    18  1140   357  1265   105  6843  5751   177    94  1459\n",
      "    957  2508   794  6269  1413   352   434  1265   143     6   256  1153\n",
      "    703     8   139    14   858    94  9710   334    15     8  1605    39\n",
      "    686   710  6390  1024   448    58   155    30     7   563   613    26\n",
      "  20839 20523   613    76  2868  8670 12241  5393    47  2872]]\n"
     ]
    }
   ],
   "source": [
    "# reviews_ints = [each for each in reviews_as_ints if len(each) > 0]\n",
    "seq_len = 250\n",
    "\n",
    "# convert reviews (as mapped ints) into numpy arrays\n",
    "# we'll use a left padding of '0's to compensate for smaller reviews\n",
    "reviews_as_feat_input = np.zeros((len(reviews_as_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_as_ints):\n",
    "    reviews_as_feat_input[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "# inspect the finalized reviews converted into usable data\n",
    "print(len(reviews_as_feat_input))\n",
    "print(reviews_as_feat_input[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "Map \n",
    " - `positive` : `1`\n",
    " - `negative` : `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "1\n",
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert to a usable format\n",
    "\n",
    "# split into single review {'positive' or 'negative'}\n",
    "labels_list = labels_raw.split('\\n')\n",
    "\n",
    "# sample: print(len(labels_list)) > 25001 | print(labels_list[25000]) > \"\"\n",
    "# same logic as above, remove empty value\n",
    "del labels_list[-1]\n",
    "# convert to numpy array and map positive=>1 and negative=>0\n",
    "# NOTE: safer method would involve making sure only 'positive' and 'negative' are present first\n",
    "labels = np.array([1 if cur_label == 'positive' else 0 for cur_label in labels_list])\n",
    "\n",
    "# print to ensure we've converted correctly\n",
    "print(len(labels))\n",
    "print(labels[2500])\n",
    "print(labels[:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 250) \n",
      "Validation set: \t(2500, 250) \n",
      "Test set: \t\t(2500, 250)\n"
     ]
    }
   ],
   "source": [
    "split_percent = 0.8\n",
    "# 80% 'training', 20% 'testing'\n",
    "# the 20% 'testing' split will be split in half;\n",
    "#    - 10% 'validation' and 10% 'testing'\n",
    "split_idx = int(len(reviews_as_feat_input)*split_percent)\n",
    "\n",
    "# split into (training and validation&testing)\n",
    "train_x, val_x = reviews_as_feat_input[:split_idx], reviews_as_feat_input[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# split validation into validation and test sets\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of where we are\n",
    "\n",
    "### Inputs\n",
    "\n",
    "Labels\n",
    "> - Converted to 0 and 1\n",
    "\n",
    "Review text\n",
    "> 1. Converted to integer representations\n",
    "> 2. Trimmed to standardized size\n",
    "> 3. Padded with 0's on the left\n",
    "\n",
    "### Split into training, validation, and testing\n",
    ">- 80% training (`train_x` and `train_y`)\n",
    ">- 10% validation (`val_x` and `val_y`)\n",
    ">- 10% testing (`test_x` and `test_y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "# number of words in our vocab\n",
    "n_words = len(vocab)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    # value for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "# `embed_size` is the size of the embedding vectors or num of units in the embedding layer\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN forward pass\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output\n",
    "with graph.as_default():\n",
    "\n",
    "    # last value from the RNN output: `outputs[:, -1]`\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation accuracy\n",
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batching\n",
    "# > NOTE: SOME DATA MAY BE REMOVED DEPENDING ON THE BATCH SIZE\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    # '//' divides then converts to int\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.243\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.224\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.320\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.197\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.188\n",
      "Val acc: 0.712\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.194\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.172\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.180\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.157\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.151\n",
      "Val acc: 0.765\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.143\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.130\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.128\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.130\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.110\n",
      "Val acc: 0.812\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.111\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.105\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.104\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.093\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.094\n",
      "Val acc: 0.824\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.091\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.084\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.076\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.076\n",
      "Epoch: 3/10 Iteration: 125 Train loss: 0.068\n",
      "Val acc: 0.832\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.079\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.077\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.075\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.065\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.073\n",
      "Val acc: 0.842\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.055\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.065\n",
      "Epoch: 4/10 Iteration: 165 Train loss: 0.057\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.073\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.053\n",
      "Val acc: 0.840\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.061\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.060\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.048\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.053\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.058\n",
      "Val acc: 0.838\n",
      "Epoch: 5/10 Iteration: 205 Train loss: 0.044\n",
      "Epoch: 5/10 Iteration: 210 Train loss: 0.053\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.040\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.047\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.049\n",
      "Val acc: 0.822\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.045\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.051\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.046\n",
      "Epoch: 6/10 Iteration: 245 Train loss: 0.050\n",
      "Epoch: 6/10 Iteration: 250 Train loss: 0.083\n",
      "Val acc: 0.806\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.054\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.103\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.064\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.036\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.028\n",
      "Val acc: 0.838\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.045\n",
      "Epoch: 7/10 Iteration: 285 Train loss: 0.031\n",
      "Epoch: 7/10 Iteration: 290 Train loss: 0.046\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.034\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.047\n",
      "Val acc: 0.796\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.032\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.024\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.029\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.030\n",
      "Epoch: 8/10 Iteration: 325 Train loss: 0.028\n",
      "Val acc: 0.816\n",
      "Epoch: 8/10 Iteration: 330 Train loss: 0.031\n",
      "Epoch: 8/10 Iteration: 335 Train loss: 0.028\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.028\n",
      "Val acc: 0.818\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.023\n",
      "Epoch: 9/10 Iteration: 365 Train loss: 0.023\n",
      "Epoch: 9/10 Iteration: 370 Train loss: 0.041\n",
      "Epoch: 9/10 Iteration: 375 Train loss: 0.031\n",
      "Val acc: 0.821\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.024\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.031\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.015\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.022\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.020\n",
      "Val acc: 0.823\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints_expanded/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create losses plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.824\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # load last checkpoint from training our model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints_expanded'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
