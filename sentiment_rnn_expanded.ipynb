{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook exands on the first notebook\n",
    " - Comments are pruned\n",
    " - More advanced NLP techniques are used during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Tensorflow version information\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check GPU presence\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import raw reviews and labels\n",
    "with open('./input_data/movie_reviews/reviews.txt', 'r') as f:\n",
    "    reviews_raw = f.read()\n",
    "with open('./input_data/movie_reviews/labels.txt', 'r') as f:\n",
    "    labels_raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review preprocessing\n",
    " - tokenize\n",
    " - remove punctuation\n",
    " - remove stopwords\n",
    " - perform stemming\n",
    " - split into individual reviews\n",
    " - convert to lowercase\n",
    " - create list of words for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizeContent(contentsRaw):\n",
    "    tokenized = nltk.tokenize.word_tokenize(contentsRaw)\n",
    "    return tokenized\n",
    "\n",
    "def removeStopWordsFromTokenized(contentsTokenized):\n",
    "    stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    filteredContents = [word for word in contentsTokenized if word not in stop_word_set]\n",
    "    return filteredContents\n",
    "\n",
    "def performPorterStemmingOnContents(contentsTokenized):\n",
    "    porterStemmer = nltk.stem.PorterStemmer()\n",
    "    filteredContents = [porterStemmer.stem(word) for word in contentsTokenized]\n",
    "    return filteredContents\n",
    "\n",
    "def removePunctuationFromTokenized(contentsTokenized):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "    \n",
    "    # manually add additional punctuation to remove\n",
    "    excludePuncuation.add('\\'\\'')\n",
    "    excludePuncuation.add('--')\n",
    "    excludePuncuation.add('``')\n",
    "\n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]\n",
    "    return filteredContents\n",
    "\n",
    "def convertItemsToLower(contentsRaw):\n",
    "    filteredContents = [term.lower() for term in contentsRaw]\n",
    "    return filteredContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrap preprocessing functions into a convenience function\n",
    "# this done becuase order *is* important\n",
    "def processData(rawContents):\n",
    "    cleaned = tokenizeContent(rawContents)\n",
    "    cleaned = removeStopWordsFromTokenized(cleaned)\n",
    "    cleaned = removePunctuationFromTokenized(cleaned)\n",
    "    cleaned = performPorterStemmingOnContents(cleaned)    \n",
    "    cleaned = convertItemsToLower(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into individual reviews (they are delimited by a '\\n')\n",
    "reviews_list = reviews_raw.split('\\n')\n",
    "# this leaves a \"\" empty value in the last index\n",
    "# sample: print(len(reviews)) > 25001 == \"\"\n",
    "# delete this last empty value\n",
    "del reviews_list[25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_reviews_list = []\n",
    "for rev in reviews_list:\n",
    "    processed_reviews_list.append(processData(rev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for r in processed_reviews_list:\n",
    "    for w in r:\n",
    "        words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(words)\n",
    "# output sample| print(word_counts) > 'Counter({'the': 336713, 'and': 164107, 'a': 163009, ....'\n",
    "\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# output sample| print(vocab) > '['the', 'and', 'a', 'of', 'to','\n",
    "\n",
    "# convert vocab to int\n",
    "# NOTE: start at 1, not 0\n",
    "vocab_to_int = {word: maping_int for maping_int, word in enumerate(vocab, 1) }\n",
    "# output sample| print(vocab_to_int) > ''together': 291, 'ewing': 26224,'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "reviews_as_ints = []\n",
    "for r in processed_reviews_list:\n",
    "    reviews_as_ints.append([vocab_to_int[term] for term in r])\n",
    "print(len(reviews_as_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare reviews (raw to converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review:\n",
      "r\n",
      "------------------\n",
      "Processed Review:\n",
      "['stori', 'man', 'unnatur', 'feel', 'pig', 'start', 'open', 'scene', 'terrif', 'exampl', 'absurd', 'comedi', 'formal', 'orchestra', 'audienc', 'turn', 'insan', 'violent', 'mob', 'crazi', 'chant', 'singer', 'unfortun', 'stay', 'absurd', 'whole', 'time', 'gener', 'narr', 'eventu', 'make', 'put', 'even', 'era', 'turn', 'cryptic', 'dialogu', 'would', 'make', 'shakespear', 'seem', 'easi', 'third', 'grader', 'technic', 'level', 'better', 'might', 'think', 'good', 'cinematographi', 'futur', 'great', 'vilmo', 'zsigmond', 'futur', 'star', 'salli', 'kirkland', 'freder', 'forrest', 'seen', 'briefli']\n",
      "------------------\n",
      "Review with terms mapped to ints:\n",
      "[13, 55, 5343, 62, 2751, 86, 246, 18, 1139, 357, 1264, 105, 6860, 5733, 177, 94, 1460, 957, 2514, 794, 6273, 1412, 352, 434, 1264, 143, 6, 256, 1155, 703, 8, 139, 14, 858, 94, 9498, 334, 15, 8, 1605, 39, 686, 710, 6429, 1023, 447, 58, 155, 30, 7, 563, 613, 26, 21845, 21185, 613, 76, 2874, 8760, 12315, 5428, 47, 2877]\n"
     ]
    }
   ],
   "source": [
    "# compare\n",
    "print(\"Raw review:\")\n",
    "print(reviews_raw[1])\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Processed Review:\")\n",
    "print(processed_reviews_list[1])\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Review with terms mapped to ints:\")\n",
    "print(reviews_as_ints[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the current state/information about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n",
      "Number reviews: 25000\n",
      "Zero-length reviews: 0\n",
      "Avg review length: 240.80784\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "review_lengths = Counter([len(each_review) for each_review in reviews_as_ints])\n",
    "print(review_lengths[130])\n",
    "\n",
    "review_len_list = list(review_lengths)\n",
    "rl_sorted = sorted(review_len_list)\n",
    "num_reviews = len(reviews_as_ints)\n",
    "avg_len = sum(val * review_lengths[val] for val in review_lengths) / num_reviews\n",
    "\n",
    "print(\"Number reviews: {}\".format(num_reviews))\n",
    "print(\"Zero-length reviews: {}\".format(review_lengths[0]))\n",
    "print(\"Avg review length: {}\".format(avg_len))\n",
    "print(\"Maximum review length: {}\".format(max(review_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a couple potential problems:\n",
    "1. The max movie length is long\n",
    "2. The average review length is also pretty large.\n",
    "    - Half of the data is larger than 240 words --> will take a long time to train a RNN\n",
    "\n",
    "### Both of these problems could be addressed by trimming the reviews\n",
    " - Will trim to `seq_len`\n",
    "\n",
    "NOTE: There is a cost to trimming our data -- we're losing some of the information of each review over n length.  This isn't ideal.  Another solution could involve removing the excessively large reviews from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 21429   308     6     3  1050   207     8  2143    32     1\n",
      "    171    57    15    49    81  5813    44   382   110   140    15  5227\n",
      "     60   154     9     1  5014  5858   475    71     5   260    12 21429\n",
      "    308    13  1982     6    74  2395     5   613    73     6  5227     1\n",
      "  24325     5  1990 10298     1  5834  1499    36    51    66   204   145\n",
      "     67  1199  5227 20929     1 45780     4     1   221   883    31  2993\n",
      "     71     4     1  5846    10   686     2    67  1499    54    10   216\n",
      "      1   384     9    62     3  1406  3707   783     5  3504   180     1\n",
      "    382    10  1212 13751    32   308     3   349   341  2925    10   143\n",
      "    127     5  7781    30     4   129  5227  1406  2327     5 21429   308\n",
      "     10   528    12   109  1450     4    60   543   102    12 21429   308\n",
      "      6   227  4163    48     3  2219    12     8   215    23]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0    63     4     3   125    36    47  7538  1397\n",
      "     16     3  4218   505    45    17     3   622   134    12     6     3\n",
      "   1279   457     4  1722   207     3 10733  7426   300     6   667    83\n",
      "     35  2117  1086  3002    34     1   901 57510     4     8    13  5146\n",
      "    464     8  2668  1722     1   221    57    17    58   794  1300   834\n",
      "    228     8    43    98   123  1469    59   147    38     1   964   142\n",
      "     29   667   123     1 13798   410    61    95  1777   306   755     5\n",
      "      3   819 10622    22     3  1726   636     8    13   128    73    21\n",
      "    233   102    17    49    50   618    34   684    85 31343 30519   684\n",
      "    374  3354 11491     2 17028  8043    51    29   108  3325]]\n"
     ]
    }
   ],
   "source": [
    "# reviews_ints = [each for each in reviews_as_ints if len(each) > 0]\n",
    "seq_len = 250\n",
    "\n",
    "# convert reviews (as mapped ints) into numpy arrays\n",
    "# we'll use a left padding of '0's to compensate for smaller reviews\n",
    "reviews_as_feat_input = np.zeros((len(reviews_as_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_as_ints):\n",
    "    reviews_as_feat_input[i, -len(row):] = np.array(row)[:seq_len]\n",
    "\n",
    "# inspect the finalized reviews converted into usable data\n",
    "print(len(reviews_as_feat_input))\n",
    "print(reviews_as_feat_input[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "Map \n",
    " - `positive` : `1`\n",
    " - `negative` : `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "1\n",
      "[1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Convert to a usable format\n",
    "\n",
    "# split into single review {'positive' or 'negative'}\n",
    "labels_list = labels_raw.split('\\n')\n",
    "\n",
    "# sample: print(len(labels_list)) > 25001 | print(labels_list[25000]) > \"\"\n",
    "# same logic as above, remove empty value\n",
    "del labels_list[-1]\n",
    "# convert to numpy array and map positive=>1 and negative=>0\n",
    "# NOTE: safer method would involve making sure only 'positive' and 'negative' are present first\n",
    "labels = np.array([1 if cur_label == 'positive' else 0 for cur_label in labels_list])\n",
    "\n",
    "# print to ensure we've converted correctly\n",
    "print(len(labels))\n",
    "print(labels[2500])\n",
    "print(labels[:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Split into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 250) \n",
      "Validation set: \t(2500, 250) \n",
      "Test set: \t\t(2500, 250)\n"
     ]
    }
   ],
   "source": [
    "split_percent = 0.8\n",
    "# 80% 'training', 20% 'testing'\n",
    "# the 20% 'testing' split will be split in half;\n",
    "#    - 10% 'validation' and 10% 'testing'\n",
    "split_idx = int(len(reviews_as_feat_input)*split_percent)\n",
    "\n",
    "# split into (training and validation&testing)\n",
    "train_x, val_x = reviews_as_feat_input[:split_idx], reviews_as_feat_input[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "# split validation into validation and test sets\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of where we are\n",
    "\n",
    "### Inputs\n",
    "\n",
    "Labels\n",
    "> - Converted to 0 and 1\n",
    "\n",
    "Review text\n",
    "> 1. Converted to integer representations\n",
    "> 2. Trimmed to standardized size\n",
    "> 3. Padded with 0's on the left\n",
    "\n",
    "### Split into training, validation, and testing\n",
    ">- 80% training (`train_x` and `train_y`)\n",
    ">- 10% validation (`val_x` and `val_y`)\n",
    ">- 10% testing (`test_x` and `test_y`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "# number of words in our vocab\n",
    "n_words = len(vocab)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    # value for dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "# `embed_size` is the size of the embedding vectors or num of units in the embedding layer\n",
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RNN forward pass\n",
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output\n",
    "with graph.as_default():\n",
    "\n",
    "    # last value from the RNN output: `outputs[:, -1]`\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)\n",
    "\n",
    "    cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation accuracy\n",
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batching\n",
    "# > NOTE: SOME DATA MAY BE REMOVED DEPENDING ON THE BATCH SIZE\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    # '//' divides then converts to int\n",
    "    n_batches = len(x)//batch_size\n",
    "    \n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.241\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.243\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.223\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.225\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.211\n",
      "Val acc: 0.664\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.189\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.178\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.200\n",
      "Epoch: 1/10 Iteration: 45 Train loss: 0.151\n",
      "Epoch: 1/10 Iteration: 50 Train loss: 0.170\n",
      "Val acc: 0.732\n",
      "Epoch: 1/10 Iteration: 55 Train loss: 0.150\n",
      "Epoch: 1/10 Iteration: 60 Train loss: 0.146\n",
      "Epoch: 1/10 Iteration: 65 Train loss: 0.149\n",
      "Epoch: 1/10 Iteration: 70 Train loss: 0.134\n",
      "Epoch: 1/10 Iteration: 75 Train loss: 0.128\n",
      "Val acc: 0.810\n",
      "Epoch: 1/10 Iteration: 80 Train loss: 0.133\n",
      "Epoch: 2/10 Iteration: 85 Train loss: 0.106\n",
      "Epoch: 2/10 Iteration: 90 Train loss: 0.134\n",
      "Epoch: 2/10 Iteration: 95 Train loss: 0.113\n",
      "Epoch: 2/10 Iteration: 100 Train loss: 0.112\n",
      "Val acc: 0.767\n",
      "Epoch: 2/10 Iteration: 105 Train loss: 0.147\n",
      "Epoch: 2/10 Iteration: 110 Train loss: 0.110\n",
      "Epoch: 2/10 Iteration: 115 Train loss: 0.117\n",
      "Epoch: 2/10 Iteration: 120 Train loss: 0.131\n",
      "Epoch: 3/10 Iteration: 125 Train loss: 0.083\n",
      "Val acc: 0.831\n",
      "Epoch: 3/10 Iteration: 130 Train loss: 0.088\n",
      "Epoch: 3/10 Iteration: 135 Train loss: 0.086\n",
      "Epoch: 3/10 Iteration: 140 Train loss: 0.140\n",
      "Epoch: 3/10 Iteration: 145 Train loss: 0.101\n",
      "Epoch: 3/10 Iteration: 150 Train loss: 0.116\n",
      "Val acc: 0.809\n",
      "Epoch: 3/10 Iteration: 155 Train loss: 0.082\n",
      "Epoch: 3/10 Iteration: 160 Train loss: 0.094\n",
      "Epoch: 4/10 Iteration: 165 Train loss: 0.106\n",
      "Epoch: 4/10 Iteration: 170 Train loss: 0.099\n",
      "Epoch: 4/10 Iteration: 175 Train loss: 0.096\n",
      "Val acc: 0.782\n",
      "Epoch: 4/10 Iteration: 180 Train loss: 0.118\n",
      "Epoch: 4/10 Iteration: 185 Train loss: 0.082\n",
      "Epoch: 4/10 Iteration: 190 Train loss: 0.081\n",
      "Epoch: 4/10 Iteration: 195 Train loss: 0.077\n",
      "Epoch: 4/10 Iteration: 200 Train loss: 0.072\n",
      "Val acc: 0.774\n",
      "Epoch: 5/10 Iteration: 205 Train loss: 0.094\n",
      "Epoch: 5/10 Iteration: 210 Train loss: 0.100\n",
      "Epoch: 5/10 Iteration: 215 Train loss: 0.082\n",
      "Epoch: 5/10 Iteration: 220 Train loss: 0.099\n",
      "Epoch: 5/10 Iteration: 225 Train loss: 0.081\n",
      "Val acc: 0.837\n",
      "Epoch: 5/10 Iteration: 230 Train loss: 0.056\n",
      "Epoch: 5/10 Iteration: 235 Train loss: 0.058\n",
      "Epoch: 5/10 Iteration: 240 Train loss: 0.069\n",
      "Epoch: 6/10 Iteration: 245 Train loss: 0.065\n",
      "Epoch: 6/10 Iteration: 250 Train loss: 0.056\n",
      "Val acc: 0.838\n",
      "Epoch: 6/10 Iteration: 255 Train loss: 0.041\n",
      "Epoch: 6/10 Iteration: 260 Train loss: 0.057\n",
      "Epoch: 6/10 Iteration: 265 Train loss: 0.051\n",
      "Epoch: 6/10 Iteration: 270 Train loss: 0.041\n",
      "Epoch: 6/10 Iteration: 275 Train loss: 0.079\n",
      "Val acc: 0.822\n",
      "Epoch: 6/10 Iteration: 280 Train loss: 0.085\n",
      "Epoch: 7/10 Iteration: 285 Train loss: 0.067\n",
      "Epoch: 7/10 Iteration: 290 Train loss: 0.095\n",
      "Epoch: 7/10 Iteration: 295 Train loss: 0.057\n",
      "Epoch: 7/10 Iteration: 300 Train loss: 0.072\n",
      "Val acc: 0.844\n",
      "Epoch: 7/10 Iteration: 305 Train loss: 0.051\n",
      "Epoch: 7/10 Iteration: 310 Train loss: 0.051\n",
      "Epoch: 7/10 Iteration: 315 Train loss: 0.065\n",
      "Epoch: 7/10 Iteration: 320 Train loss: 0.060\n",
      "Epoch: 8/10 Iteration: 325 Train loss: 0.040\n",
      "Val acc: 0.800\n",
      "Epoch: 8/10 Iteration: 330 Train loss: 0.055\n",
      "Epoch: 8/10 Iteration: 335 Train loss: 0.034\n",
      "Epoch: 8/10 Iteration: 340 Train loss: 0.044\n",
      "Epoch: 8/10 Iteration: 345 Train loss: 0.037\n",
      "Epoch: 8/10 Iteration: 350 Train loss: 0.033\n",
      "Val acc: 0.818\n",
      "Epoch: 8/10 Iteration: 355 Train loss: 0.036\n",
      "Epoch: 8/10 Iteration: 360 Train loss: 0.044\n",
      "Epoch: 9/10 Iteration: 365 Train loss: 0.028\n",
      "Epoch: 9/10 Iteration: 370 Train loss: 0.044\n",
      "Epoch: 9/10 Iteration: 375 Train loss: 0.027\n",
      "Val acc: 0.824\n",
      "Epoch: 9/10 Iteration: 380 Train loss: 0.051\n",
      "Epoch: 9/10 Iteration: 385 Train loss: 0.046\n",
      "Epoch: 9/10 Iteration: 390 Train loss: 0.029\n",
      "Epoch: 9/10 Iteration: 395 Train loss: 0.070\n",
      "Epoch: 9/10 Iteration: 400 Train loss: 0.026\n",
      "Val acc: 0.845\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y[:, None],\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y[:, None],\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment_expanded.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: create losses plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\sentiment.ckpt\n",
      "Test accuracy: 0.840\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # load last checkpoint from training our model\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                labels_: y[:, None],\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
